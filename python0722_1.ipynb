{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1595203740322",
   "display_name": "Python 3.7.7 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 자연어 처리를 위한 라이브러리 설치\n",
    "\n",
    "1) 텐서플로우  \n",
    "2) 케라스  \n",
    "3) 젠심(gensim)  \n",
    "4) 사이킷런  \n",
    "5) nltk(영문관련한 자연어처리)  \n",
    "6) kolnpy  \n",
    "7) jpype   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastery', 'shop']\n"
    }
   ],
   "source": [
    "print(word_tokenize(\"\"\"Don't be fooled by the dark sounding name, Mr.jone's Orphanage is as cheery as cheery goes for a pastery shop\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastery', 'shop']\n"
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "print(wordpunct_tokenize(\"\"\"Don't be fooled by the dark sounding name, Mr.jone's Orphanage is as cheery as cheery goes for a pastery shop\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[\"don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', \"jone's\", 'orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastery', 'shop']\n"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "print(text_to_word_sequence(\"\"\"\"Don't be fooled by the dark sounding name, Mr.jone's Orphanage is as cheery as cheery goes for a pastery shop\"\"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문장 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"I am actively looking for Ph.D. students. and you are a Ph.D student.\"\n",
    "text2 = \"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to mae sure no one was near.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['His barber kept his word.',\n 'But keeping such a huge secret to himself was driving him crazy.',\n 'Finally, the barber went up a mountain and almost to the edge of a cliff.',\n 'He dug a hole in the midst of some reeds.',\n 'He looked about, to mae sure no one was near.']"
     },
     "metadata": {},
     "execution_count": 236
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokenize(text1)\n",
    "sent_tokenize(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# token\n",
    "토큰이란? 분석을 하기 위한 수준으로 나누어주는 단위  \n",
    "문자, 문장, 등 단위는 사용자가 원하는 수준으로 나눠짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'and', 'you', 'are', 'a', 'Ph.D', 'student', '.']\n"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(text1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('I', 'PRP'),\n ('am', 'VBP'),\n ('actively', 'RB'),\n ('looking', 'VBG'),\n ('for', 'IN'),\n ('Ph.D.', 'NNP'),\n ('students', 'NNS'),\n ('.', '.'),\n ('and', 'CC'),\n ('you', 'PRP'),\n ('are', 'VBP'),\n ('a', 'DT'),\n ('Ph.D', 'NNP'),\n ('student', 'NN'),\n ('.', '.')]"
     },
     "metadata": {},
     "execution_count": 238
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "imsi = word_tokenize(text1)\n",
    "pos_tag(imsi) # 품사 단위로 출력이 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 형태소 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['공부', '우리', '취업']"
     },
     "metadata": {},
     "execution_count": 239
    }
   ],
   "source": [
    "from konlpy.tag import Okt #형태소를 출력\n",
    "okt = Okt()\n",
    "okt.morphs(\"열심히 공부한 우리. 취업에 성공합시다\") # 형태소\n",
    "okt.pos(\"열심히 공부한 우리. 취업에 성공합시다\") \n",
    "okt.nouns(\"열심히 공부한 우리. 취업에 성공합시다\") # 명사\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 형태소 어간 추출  \n",
    "어간(stem, 의미), 접사(부가적인 의미)   \n",
    "dogs => dog(어간), -s(접사), cat(어간)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "having\n"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wn = WordNetLemmatizer()\n",
    "words = ['loves', 'dies', 'complete', 'having']\n",
    "print(wn.lemmatize(words[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'have'"
     },
     "metadata": {},
     "execution_count": 241
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer() # 어간 추출(품사 정보가 상실)\n",
    "ps.stem(words[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
    }
   ],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ex에 저장된 단어에서 불용어를 제거하고 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = 'Family is not an important thing. It;s everything.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', ';', 's', 'everything', '.']\n['Family', 'important', 'thing', '.', 'It', ';', 'everything', '.']\n"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "word_token = word_tokenize(ex)\n",
    "\n",
    "result = []\n",
    "for w in word_token:\n",
    "    if w not in stop_words:\n",
    "        result.append(w)\n",
    "\n",
    "print(word_token)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['Family', 'is', 'not', 'an', 'important', 'thing', 'It', 's', 'everything']\n"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tok = RegexpTokenizer(\"[\\w]+\") # 정규표현식 사용 \n",
    "# \\w = [a-zA-z]]\n",
    "print(tok.tokenize('Family is not an important thing. It;s everything.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문장 단위 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['A barber is a person.', 'a barber is good person.', 'a barber is huge person.', 'he Knew A Secret!', 'The Secret He Kept is huge secret.', 'Huge secret.', 'His barber kept his word.', 'a barber kept his word.', 'His barber kept his secret.', 'But keeping and keeping such a huge secret to himself was driving the barber crazy.', 'the barber went up a huge mountain.']\n"
    }
   ],
   "source": [
    "text = sent_tokenize(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "11\n"
    }
   ],
   "source": [
    "print(len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11개의 문장 각각에 대해, 단어 토큰화 수행\n",
    "1) 불용어는 제거  \n",
    "2) 단어 길이가 2이하면 제거  \n",
    "3) 모든 단어는 소문자화  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['the', 'secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['his', 'barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['his', 'barber', 'kept', 'secret'], ['but', 'keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "res = []\n",
    "\n",
    "for i in range(0, len(text)):\n",
    "    word_token = word_tokenize(text[i])\n",
    "    result = []\n",
    "    for w in word_token:\n",
    "        if w not in stop_words and len(w) > 2:\n",
    "            w = w.lower()\n",
    "            result.append(w)\n",
    "    res.append(result)\n",
    "# print(word_token)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
    }
   ],
   "source": [
    "vocab = {} \n",
    "sentences = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "for i in text:\n",
    "    sentence = word_tokenize(i) \n",
    "    result = []\n",
    "    for word in sentence: \n",
    "        word = word.lower() \n",
    "        if word not in stop_words:\n",
    "            if len(word) > 2:\n",
    "                result.append(word)\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = 0 \n",
    "                vocab[word] += 1\n",
    "    sentences.append(result) \n",
    "print(sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 빈도수를 기준으로 내림차순 출력(sorted 함수))\n",
    "# 츨력예시\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = sorted(vocab.items(), key=lambda x : x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7}\n"
    }
   ],
   "source": [
    "wti = {}\n",
    "i = 0\n",
    "for (word,f) in vs:\n",
    "    if f>1 :\n",
    "        i += 1\n",
    "        wti[word] = i\n",
    "print(wti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = okt.morphs('오전에는 졸았지만, 오후에는 열심히 하겠다.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토큰 단위로 딕셔너리에 저장하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'오전': 0, '에는': 1, '졸았지만': 2, ',': 3, '오후': 4, '열심히': 5, '하겠다': 6, '.': 7}\n"
    }
   ],
   "source": [
    "word2index = {}\n",
    "\n",
    "for v in token:\n",
    "    if v not in word2index.keys(): # v 토큰이 word2 index에 없다면?\n",
    "        word2index[v] = len(word2index)\n",
    "print(word2index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 케라스 토크나이져"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0, 0, 0, 0, 1, 0, 0, 0]\n"
    }
   ],
   "source": [
    "def ohe(w, word2index):\n",
    "    ohv = [0]*len(word2index)\n",
    "    idx = word2index[w]\n",
    "    ohv[idx] = 1\n",
    "    return ohv\n",
    "print(ohe('오후', word2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# text = '문일주 공부 좀 해, 너 요즘 놀고 있잖아'\n",
    "from keras_preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "text = \"\"\"나랑 막걸리 마시러 갈래 안주는 파전 갈래 갈래 파전 최고야\"\"\" # 이게 코퍼스임!\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts([text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0., 0., 0., 0., 0., 1.],\n       [0., 1., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 0.]], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 258
    }
   ],
   "source": [
    "t.word_index # 형태소 분석을 해서 인덱스 먹임\n",
    "t.index_word\n",
    "enc = t.texts_to_sequences(['술 마시러 갈래 갈꺼야 말꺼야 갈래'])[0]\n",
    "# 코퍼스로부터, 만들어 낸거 \n",
    "enc # 2차원\n",
    "to_categorical(enc) #차원이 하나가 늘어나서 저장\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bag of words\n",
    "단어들이 들어가 있는 가방  \n",
    "각 단어에 인덱스 부여 -> 등장 횟수 저장\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 마침표 제거하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# re.sub(패턴, 교체문자, 데이터)\n",
    "\n",
    "token = re.sub(\"[.]+\",'',\"정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.\")\n",
    "token = re.sub(\"(\\.)+\",'',\"정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다'"
     },
     "metadata": {},
     "execution_count": 260
    }
   ],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = okt.morphs(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'정부': 0, '가': 1, '발표': 2, '하는': 3, '물가상승률': 4, '과': 5, '소비자': 6, '느끼는': 7, '은': 8, '다르다': 9}\n====================\n[1, 2, 1, 1, 2, 1, 1, 1, 1, 1]\n"
    }
   ],
   "source": [
    "bow = []\n",
    "word2index = {}\n",
    "\n",
    "for v in token:\n",
    "    if v not in word2index.keys():\n",
    "        # print(v)\n",
    "        word2index[v] = len(word2index)  #{'정부' : 0, '가':1}\n",
    "        bow.insert(len(word2index)-1, 1) # 0으로 각 자리를 초기화\n",
    "    else: \n",
    "        idx = word2index.get(v)\n",
    "        bow[idx] = bow[idx]+1\n",
    "print(word2index)\n",
    "print('='*20)\n",
    "print(bow)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "insert 함수를 이용해서 원하는 위치에 추가할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[1 1 2 1 2 1]]\n{'you': 4, 'know': 1, 'want': 3, 'your': 5, 'love': 2, 'because': 0}\n"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = ['you know I want your love. because I love you']\n",
    "\n",
    "vec = CountVectorizer()\n",
    "# 각 단어의 빈도수\n",
    "print(vec.fit_transform(corpus).toarray())\n",
    "# 각 단어의 인덱스\n",
    "print(vec.vocabulary_) # 인덱스는 알파벳 순으로 매김\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[1 1 1 2 1]]\n{'you': 3, 'know': 1, 'want': 2, 'your': 4, 'because': 0}\n"
    }
   ],
   "source": [
    "corpus = ['you know I want your love. because I love you']\n",
    "\n",
    "vec = CountVectorizer(stop_words=['love'])  # 사용자가 직접 불용어를 지정할 경우 \n",
    "# 각 단어의 빈도수\n",
    "print(vec.fit_transform(corpus).toarray())\n",
    "# 각 단어의 인덱스\n",
    "print(vec.vocabulary_) # 인덱스는 알파벳 순으로 매김\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = ['먹고 싶은 사과', '먹고 싶은 바나나', '길고 노란 바나나 바나나', '저는 과일이 좋아요']\n",
    "\n",
    "voc = list(set(w for doc in docs for w in doc.split()))\n",
    "# <------------->\n",
    "# 먹고싶은 사과 \n",
    "#                 <------------------>\n",
    "#                    먹고/ 싶은/ 사과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(docs)\n",
    "def tfv(t, d):\n",
    "    return d.count(t) # d문서에서 t 단어의 등장횟수\n",
    "def idf(t):\n",
    "    df = 0\n",
    "    for doc in docs:\n",
    "        df += t in doc\n",
    "    return log(n/(df+1))\n",
    "\n",
    "def tfidf(t,d):\n",
    "    return tfv(t,d)*idf(t)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   과일이  길고  노란  먹고  바나나  사과  싶은  저는  좋아요\n0    0   0   0   1    0   1   1   0    0\n1    0   0   0   1    1   0   1   0    0\n2    0   1   1   0    2   0   0   0    0\n3    1   0   0   0    0   0   0   1    1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>과일이</th>\n      <th>길고</th>\n      <th>노란</th>\n      <th>먹고</th>\n      <th>바나나</th>\n      <th>사과</th>\n      <th>싶은</th>\n      <th>저는</th>\n      <th>좋아요</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 289
    }
   ],
   "source": [
    "res = []\n",
    "for i in range(n):\n",
    "    res.append([])\n",
    "    d =docs[i]\n",
    "    for j in range(len(voc)):\n",
    "        t= voc[j]\n",
    "        res[-1].append(tfv(t,d))\n",
    "mytf = pd.DataFrame(res, columns = voc)\n",
    "mytf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for j in range(len(voc)):\n",
    "    t= voc[j]\n",
    "    res[-1].append(idf(t))\n",
    "mytf = pd.DataFrame(res, columns = voc)\n",
    "mytf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "        과일이        길고        노란  ...        싶은        저는       좋아요\n0  0.000000  0.000000  0.000000  ...  0.287682  0.000000  0.000000\n1  0.000000  0.000000  0.000000  ...  0.287682  0.000000  0.000000\n2  0.000000  0.693147  0.693147  ...  0.000000  0.000000  0.000000\n3  0.693147  0.000000  0.000000  ...  0.000000  0.693147  0.693147\n\n[4 rows x 9 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>과일이</th>\n      <th>길고</th>\n      <th>노란</th>\n      <th>먹고</th>\n      <th>바나나</th>\n      <th>사과</th>\n      <th>싶은</th>\n      <th>저는</th>\n      <th>좋아요</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.287682</td>\n      <td>0.000000</td>\n      <td>0.693147</td>\n      <td>0.287682</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.287682</td>\n      <td>0.287682</td>\n      <td>0.000000</td>\n      <td>0.287682</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.000000</td>\n      <td>0.693147</td>\n      <td>0.693147</td>\n      <td>0.000000</td>\n      <td>0.575364</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.693147</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.693147</td>\n      <td>0.693147</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 296
    }
   ],
   "source": [
    "res = []\n",
    "for i in range(n):\n",
    "    res.append([])\n",
    "    d = docs[i]\n",
    "    for j in range(len(voc)):\n",
    "        t = voc[j]\n",
    "        res[-1].append(tfidf(t, d))\n",
    "mytfidf = pd.DataFrame(res, columns = voc)\n",
    "mytfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[0 1 0 1 0 1 0 2]\n [0 0 1 0 0 0 0 1]\n [1 0 0 0 1 0 1 0]]\n{'you': 7, 'know': 1, 'want': 5, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}\n"
    }
   ],
   "source": [
    "corpus= ['you know I want you love', 'I like you', ' what should I do']\n",
    "\n",
    "# DTM \n",
    "vector = CountVectorizer()\n",
    "print(vector.fit_transform(corpus).toarray())\n",
    "print(vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[0.         0.43381609 0.         0.43381609 0.         0.43381609\n  0.         0.65985664]\n [0.         0.         0.79596054 0.         0.         0.\n  0.         0.60534851]\n [0.57735027 0.         0.         0.         0.57735027 0.\n  0.57735027 0.        ]]\n"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidfv = TfidfVectorizer().fit(corpus)\n",
    "print(tfidfv.transform(corpus).toarray())"
   ]
  }
 ]
}